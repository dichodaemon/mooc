{
 "metadata": {
  "name": "",
  "notebookname": "Clustering with Expectation-Maximization",
  "signature": "sha256:9c84994830e4d893b6d10f930aa397b704151a68642b81a9c349a306c672f75f",
  "version": "1.0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Clustering with Expectation-Maximization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Single Gaussian source"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's first import the required modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy as np\n",
      "import pylab as pl\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's assume we have some sort of two-dimensional data source $S_1$, whose output depends on a multivariate Gaussian distribution:\n",
      "\n",
      "$N( \\mu_1, \\Sigma_1 )$\n",
      "\n",
      "Let us define both parameters, the mean $\\mu_1$ and the covariance $\\Sigma_1$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean = [5, 5]\n",
      "covariance = [[1.0, 0.0], [0.0, 1.0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can sample some points (1000 in this example) from our distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points = np.random.multivariate_normal( mean, covariance, 1000 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And check the first 20 points:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's not easy to understand, what about plotting it?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.plot( points[:,0], points[:,1], \".\" )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But, what if we are in the opposite case, having the data $X$, but not the Gaussian's parameters? It is very straigthforward to compute the mean using the following expression: \n",
      "\n",
      "$\\hat{\\mu} = \\frac{1}{N} \\sum^N_{i=1} X_i$\n",
      "\n",
      "In Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "empirical_mean = sum( points, 0 ) / len( points )\n",
      "empirical_mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which is reasonably close to our \"real\" mean value. Similarly, for the covariance, we use the following expression:\n",
      "\n",
      "$$\\hat{\\Sigma} = \\frac{1}{N - 1}x'x$$\n",
      "\n",
      "Where the lower-case $x$ is the deviation score, obtained by subtracting, for each point, the empirical mean from its original value.\n",
      "\n",
      "Translating to python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deviation_scores = points[:] - empirical_mean\n",
      "empirical_covariance = np.dot( deviation_scores.transpose(), deviation_scores ) / \\\n",
      "                      ( len( points ) - 1 )\n",
      "empirical_covariance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which, again, is not too different from our \"real\" covariance matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multiple Gaussian sources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What if we have three Gaussian sources? Let's assume that the covariance is the same for all the three and generate the mean values at random:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "means = np.random.uniform( 0, 10, ( 3, 2 ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points1 = np.random.multivariate_normal( means[0], covariance, 1000 )\n",
      "points2 = np.random.multivariate_normal( means[1], covariance, 1000 )\n",
      "points3 = np.random.multivariate_normal( means[2], covariance, 1000 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And, plotting them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.plot( points1[:, 0], points1[:, 1], \"b.\" )\n",
      "pl.plot( points2[:, 0], points2[:, 1], \"r.\" )\n",
      "pl.plot( points3[:, 0], points3[:, 1], \"g.\" )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now for the inverse case. It is clear that, since we are able to identify the elements of each source, it would be straightforward to compute the individual source parameters, as we did for the individual case. But, what if we were not able to do that?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_points = np.concatenate( ( points1, points2, points3 ) )\n",
      "pl.plot( all_points[:, 0], all_points[:, 1], \"k.\" )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def expectation( data, means, covariance, f ):\n",
      "    '''Expectation step'''\n",
      "    N = len( data )  # Number of trajectories\n",
      "    M = len( means ) # Number of clusters\n",
      "    e = np.zeros( ( N, M ) ) # Expectation array\n",
      "    for n in xrange( N ):\n",
      "        for m in xrange( M ):\n",
      "            e[n, m] = f( means[m], covariance, data[n] )\n",
      "        e[n, :] /= np.sum( e[n, :] )\n",
      "    return e\n",
      "\n",
      "def maximization( data, e, means, zero, cummulate ):\n",
      "    '''Maximization step'''\n",
      "    N = len( data )\n",
      "    M = len( means )\n",
      "    for m in xrange( M ):\n",
      "        zero( means[m] )\n",
      "        for n in xrange( N ):\n",
      "            cummulate( e[n, m], means[m], data[n] )\n",
      "        means[m] /= np.sum( e[:, m] ) # Normalize        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def u_gaussian_pdf( mean, covariance, value ):\n",
      "    '''Unnormalized Gaussian PDF'''\n",
      "    diff = value - mean\n",
      "    inv = np.linalg.inv( covariance )\n",
      "    return np.exp( - np.dot( np.dot( diff, inv ), diff.transpose() ) )\n",
      "\n",
      "def v_zero( value ):\n",
      "    '''Set vector to zero'''\n",
      "    value[:] = 0\n",
      "\n",
      "def v_cummulate( weight, v1, v2 ):\n",
      "    '''Cummulate weighted vector v2 into v1'''\n",
      "    v1 += weight * v2\n",
      "    \n",
      "def plot_expectations( data, expectations ):\n",
      "    colors = [\"r.\", \"g.\", \"b.\"]\n",
      "    max_expectations = expectations.transpose().max( axis = 0 )\n",
      "    indicators = ( expectations.transpose() >= max_expectations ).transpose()\n",
      "    for i in xrange( expectations.shape[1] ):\n",
      "        pl.plot( data[indicators[:, i], 0], data[indicators[:, i], 1], colors[i] )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est_means = np.random.uniform( 0, 10, ( 3, 2 ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can iterate through the algorithm in order to see how it evolves (use ctrl-enter in the cell below)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expectations = expectation( all_points, est_means, covariance, u_gaussian_pdf )\n",
      "maximization( all_points, expectations, est_means, v_zero, v_cummulate )\n",
      "plot_expectations( all_points, expectations )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}